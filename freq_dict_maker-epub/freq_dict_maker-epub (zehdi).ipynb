{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ebooklib \n",
    "from ebooklib import epub\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_epub(file_path):\n",
    "    book = epub.read_epub(file_path)\n",
    "    content = []\n",
    "\n",
    "    for item in book.get_items():\n",
    "        if item.get_type() == ebooklib.ITEM_DOCUMENT:\n",
    "            soup = BeautifulSoup(item.get_body_content(), 'html.parser')\n",
    "            content.append(soup.get_text())\n",
    "\n",
    "    return '\\n'.join(content)\n",
    "\n",
    "file_path = '無職転生 ～異世界行ったら本気だす～.epub'\n",
    "epub_content = read_epub(file_path)\n",
    "\n",
    "epub_sentences = epub_content.split()\n",
    "\n",
    "japanese_char_pattern = re.compile(r'[\\u3040-\\u309F\\u30a0-\\u30ff\\u4e00-\\u9faf]')\n",
    "filtered_sentences = []\n",
    "for sentence in epub_sentences:\n",
    "    filtered_sentence = ''.join(char for char in sentence if japanese_char_pattern.match(char))\n",
    "    if filtered_sentence: \n",
    "        filtered_sentences.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize/Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Pure Kanji Words: 2623\n",
      "Number of Kanji + Kana Words: 1707\n",
      "Number of Kana Words: 1496\n",
      "Total Unique Words: 5826\n",
      "Total Words: 63158\n"
     ]
    }
   ],
   "source": [
    "tokenizer_obj = dictionary.Dictionary(dict=\"full\").create(tokenizer.Tokenizer.SplitMode.C)\n",
    "\n",
    "def is_kanji(char):\n",
    "    return '\\u4e00' <= char <= '\\u9faf'\n",
    "\n",
    "def is_pure_kanji(token):\n",
    "    return all(is_kanji(char) for char in token)\n",
    "\n",
    "def contains_kanji(token):\n",
    "    return any(is_kanji(char) for char in token)\n",
    "\n",
    "pure_kanji_freq = defaultdict(int)\n",
    "kanji_kana_freq = defaultdict(int)\n",
    "kana_freq = defaultdict(int)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [word.dictionary_form() for word in tokenizer_obj.tokenize(text)]\n",
    "\n",
    "for sentence in filtered_sentences:\n",
    "    tokens = tokenize_text(sentence)\n",
    "    for token in tokens:\n",
    "        if not token:\n",
    "            continue\n",
    "        if is_pure_kanji(token):\n",
    "            pure_kanji_freq[token] += 1\n",
    "        elif contains_kanji(token):\n",
    "            kanji_kana_freq[token] += 1\n",
    "        else:\n",
    "            kana_freq[token] += 1\n",
    "\n",
    "kanji_words = sorted(pure_kanji_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "kanji_kana_words = sorted(kanji_kana_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "kana_words = sorted(kana_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"Number of Pure Kanji Words:\", len(kanji_words))\n",
    "print(\"Number of Kanji + Kana Words:\", len(kanji_kana_words))\n",
    "print(\"Number of Kana Words:\", len(kana_words))\n",
    "\n",
    "all_vocab = kanji_words + kanji_kana_words + kana_words\n",
    "sorted_occurences = sorted(all_vocab, key=lambda x: x[1], reverse=True)\n",
    "print(f'Total Unique Words: {len(sorted_occurences)}')\n",
    "\n",
    "total_occurrences = sum(count for word, count in (kana_words + kanji_kana_words + kanji_words))\n",
    "print(f'Total Words: {total_occurrences}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('言う', 237),\n",
       " ('思う', 219),\n",
       " ('見る', 158),\n",
       " ('俺の', 113),\n",
       " ('使う', 98),\n",
       " ('聞く', 91),\n",
       " ('出る', 82),\n",
       " ('教える', 62),\n",
       " ('持つ', 56),\n",
       " ('知る', 53)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kanji_kana_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n",
      "New JSON file term_meta_bank_1.json has been created.\n"
     ]
    }
   ],
   "source": [
    "JPDB_freq = \"./JPDB_term_meta_bank_1.json\"\n",
    "\n",
    "all_vocab = sorted(sorted_occurences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "new_frequencies = {word: i + 1 for i, (word, _) in enumerate(all_vocab)}\n",
    "\n",
    "with open(JPDB_freq, 'r', encoding='utf-8') as file:\n",
    "    jpdb_data = json.load(file)\n",
    "\n",
    "filtered_data = []\n",
    "for item in jpdb_data:\n",
    "    word = item[0]\n",
    "    if word in new_frequencies:\n",
    "        filtered_data.append(item)\n",
    "\n",
    "def update_frequencies(data, freq_dict):\n",
    "    for item in data:\n",
    "        word = item[0]\n",
    "        reading = item[2].get('reading') if 'reading' in item[2] else None\n",
    "        if word in freq_dict:\n",
    "            freq = freq_dict[word]\n",
    "        elif reading in freq_dict:\n",
    "            freq = freq_dict[reading]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if 'frequency' in item[2]:\n",
    "          item[2]['frequency'] = {\n",
    "              'value': freq,\n",
    "              'displayValue': f\"{freq}㋕\"\n",
    "          }\n",
    "print('Success!')\n",
    "\n",
    "update_frequencies(filtered_data, new_frequencies)\n",
    "\n",
    "sorted_data = sorted(filtered_data, key=lambda x: x[2]['frequency']['value'] if 'frequency' in x[2] else x[2]['value'])\n",
    "\n",
    "term_meta_json = 'term_meta_bank_1.json'\n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(sorted_data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"New JSON file {term_meta_json} has been created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2801 duplicates successfully!\n"
     ]
    }
   ],
   "source": [
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "unique_data = []\n",
    "delete_counter = 0\n",
    "for i in range(len(term_data)):\n",
    "    is_duplicate = False\n",
    "    for j in range(i + 1, len(term_data)):\n",
    "        if term_data[i] == term_data[j]:\n",
    "            is_duplicate = True\n",
    "            delete_counter += 1\n",
    "            break\n",
    "    if not is_duplicate:\n",
    "        unique_data.append(term_data[i])\n",
    "\n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(unique_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Removed {delete_counter} duplicates successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurrence(word):\n",
    "    for item in sorted_occurences:\n",
    "        if item[0] == word:\n",
    "            return item[1]\n",
    "    return None\n",
    "\n",
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "term_data.sort(key=lambda x: x[2]['frequency']['value'] if 'frequency' in x[2] else x[2]['value'])\n",
    "\n",
    "for index, term in enumerate(term_data):\n",
    "    if 'frequency' in term[2]:\n",
    "      term[2]['frequency']['value'] = index + 1\n",
    "      term[2]['frequency']['displayValue'] = f\"{find_occurrence(term[0])}\"\n",
    "    else:\n",
    "      term[2]['value'] = index + 1\n",
    "      term[2]['displayValue'] = f\"{find_occurrence(term[0])}\"\n",
    "        \n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(term_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "term_data.sort(key=lambda x: (int(x[2]['frequency']['displayValue']) if 'frequency' in x[2] else int(x[2]['displayValue'])), reverse=True)\n",
    "\n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(term_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term_meta_bank_1.json is ready for use!\n"
     ]
    }
   ],
   "source": [
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "for index, term in enumerate(term_data):\n",
    "    if 'frequency' in term[2]:\n",
    "      term[2]['frequency']['value'] = index + 1\n",
    "      term[2]['frequency']['displayValue'] = f\"x{term[2]['frequency']['displayValue']}\"\n",
    "    else:\n",
    "      term[2]['value'] = index + 1\n",
    "      term[2]['displayValue'] = f\"x{term[2]['displayValue']}\"\n",
    "        \n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(term_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f'{term_meta_json} is ready for use!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
