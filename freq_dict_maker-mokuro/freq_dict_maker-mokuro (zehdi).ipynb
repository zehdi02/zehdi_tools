{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "from collections import defaultdict\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir('.')\n",
    "pattern2 = r\"^VOL +\"\n",
    "matching_folders = [folder for folder in file_list if re.match(pattern2, folder)]\n",
    "\n",
    "sorted(matching_folders)\n",
    "print(f'Found {len(matching_folders)} folders containing text.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "\n",
    "japanese_char_pattern = re.compile(r'[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FAF]')\n",
    "\n",
    "for i in range(len(matching_folders)):\n",
    "    folder_path = matching_folders[i]\n",
    "\n",
    "    json_files = sorted(glob.glob(os.path.join(folder_path, '*.json')))\n",
    "\n",
    "    for json_fp in json_files:\n",
    "        with open(json_fp, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        lines = [[''.join(block[\"lines\"])] for block in data.get(\"blocks\", [])]\n",
    "\n",
    "        cleaned_lines = [[\n",
    "            ''.join(japanese_char_pattern.findall(line)) for line in sentence\n",
    "        ] for sentence in lines]\n",
    "\n",
    "        cleaned_lines = [sentence for sentence in cleaned_lines if any(sentence)]\n",
    "\n",
    "        all_sentences.extend(cleaned_lines)\n",
    "        \n",
    "    if (i % 5) == 0:\n",
    "      print(f'Finished processing {matching_folders[i]}')\n",
    "\n",
    "print(f'Found {len(all_sentences)} sentences across {len(matching_folders)} folders.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make frequency list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_obj = dictionary.Dictionary(dict=\"full\").create(tokenizer.Tokenizer.SplitMode.C)\n",
    "\n",
    "def is_kanji(char):\n",
    "    return '\\u4e00' <= char <= '\\u9faf'\n",
    "\n",
    "def is_pure_kanji(token):\n",
    "    return all(is_kanji(char) for char in token)\n",
    "\n",
    "def contains_kanji(token):\n",
    "    return any(is_kanji(char) for char in token)\n",
    "\n",
    "vocab_freq = defaultdict(int)\n",
    "kana_words_freq = defaultdict(int)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return [word.surface() for word in tokenizer_obj.tokenize(text)]\n",
    "\n",
    "for sentence_list in all_sentences:\n",
    "    for sentence in sentence_list:\n",
    "        tokens = tokenize_text(sentence)\n",
    "        for token in tokens:\n",
    "            if not token:\n",
    "                continue\n",
    "            if is_pure_kanji(token):\n",
    "                vocab_freq[token] += 1\n",
    "            elif contains_kanji(token):\n",
    "                vocab_freq[token] += 1\n",
    "            else:\n",
    "                kana_words_freq[token] += 1\n",
    "\n",
    "kanji_words = sorted(vocab_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "kana_words = sorted(kana_words_freq.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "print(\"Number of Unique Vocabulary:\", len(kanji_words))\n",
    "print(\"Number of Removed Words:\", len(kana_words))\n",
    "\n",
    "all_vocab = kanji_words + kana_words\n",
    "sorted_occurences = sorted(all_vocab, key=lambda x: x[1], reverse=True)\n",
    "print(f'Total Unique Words: {len(sorted_occurences)}')\n",
    "\n",
    "total_occurrences = sum(count for word, count in kana_words + kanji_words)\n",
    "print(f'Total Words: {total_occurrences}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JPDB_freq = \"./JPDB_term_meta_bank_1.json\"\n",
    "\n",
    "all_vocab = sorted(sorted_occurences, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "new_frequencies = {word: i + 1 for i, (word, _) in enumerate(all_vocab)}\n",
    "\n",
    "with open(JPDB_freq, 'r', encoding='utf-8') as file:\n",
    "    jpdb_data = json.load(file)\n",
    "\n",
    "filtered_data = []\n",
    "for item in jpdb_data:\n",
    "    word = item[0]\n",
    "    if word in new_frequencies:\n",
    "        filtered_data.append(item)\n",
    "\n",
    "def update_frequencies(data, freq_dict):\n",
    "    for item in data:\n",
    "        word = item[0]\n",
    "        reading = item[2].get('reading') if 'reading' in item[2] else None\n",
    "        if word in freq_dict:\n",
    "            freq = freq_dict[word]\n",
    "        elif reading in freq_dict:\n",
    "            freq = freq_dict[reading]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        if 'frequency' in item[2]:\n",
    "          item[2]['frequency'] = {\n",
    "              'value': freq,\n",
    "              'displayValue': f\"{freq}ã‹•\"\n",
    "          }\n",
    "print('Success!')\n",
    "\n",
    "update_frequencies(filtered_data, new_frequencies)\n",
    "\n",
    "sorted_data = sorted(filtered_data, key=lambda x: x[2]['frequency']['value'] if 'frequency' in x[2] else x[2]['value'])\n",
    "\n",
    "term_meta_json = 'term_meta_bank_1.json'\n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(sorted_data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"New JSON file {term_meta_json} has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete dupes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "unique_data = []\n",
    "delete_counter = 0\n",
    "for i in range(len(term_data)):\n",
    "    is_duplicate = False\n",
    "    for j in range(i + 1, len(term_data)):\n",
    "        if term_data[i] == term_data[j]:\n",
    "            is_duplicate = True\n",
    "            delete_counter += 1\n",
    "            break\n",
    "    if not is_duplicate:\n",
    "        unique_data.append(term_data[i])\n",
    "\n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(unique_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Removed {delete_counter} duplicates successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reorder terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_occurrence(word):\n",
    "    for item in sorted_occurences:\n",
    "        if item[0] == word:\n",
    "            return item[1]\n",
    "    return None\n",
    "\n",
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "term_data.sort(key=lambda x: x[2]['frequency']['value'] if 'frequency' in x[2] else x[2]['value'])\n",
    "\n",
    "for index, term in enumerate(term_data):\n",
    "    if 'frequency' in term[2]:\n",
    "      term[2]['frequency']['value'] = index + 1\n",
    "      term[2]['frequency']['displayValue'] = f\"{find_occurrence(term[0])}\"\n",
    "    else:\n",
    "      term[2]['value'] = index + 1\n",
    "      term[2]['displayValue'] = f\"{find_occurrence(term[0])}\"\n",
    "        \n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(term_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort json by word frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "term_data.sort(key=lambda x: (int(x[2]['frequency']['displayValue']) if 'frequency' in x[2] else int(x[2]['displayValue'])), reverse=True)\n",
    "\n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(term_data, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add marker for displayValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(term_meta_json, 'r', encoding='utf-8') as file:\n",
    "    term_data = json.load(file)\n",
    "\n",
    "for index, term in enumerate(term_data):\n",
    "    if 'frequency' in term[2]:\n",
    "      term[2]['frequency']['value'] = index + 1\n",
    "      term[2]['frequency']['displayValue'] = f\"x{term[2]['frequency']['displayValue']}\"\n",
    "    else:\n",
    "      term[2]['value'] = index + 1\n",
    "      term[2]['displayValue'] = f\"x{term[2]['displayValue']}\"\n",
    "        \n",
    "with open(term_meta_json, 'w', encoding='utf-8') as file:\n",
    "    json.dump(term_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f'{term_meta_json} is ready for use!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
